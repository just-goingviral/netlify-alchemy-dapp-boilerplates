{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/just-goingviral/netlify-alchemy-dapp-boilerplates/blob/main/Copy_of_crewai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt-YbbQKlcml"
      },
      "outputs": [],
      "source": [
        "# Use \"pip\" to install the \"crewai\" library.\n",
        "# This lets us builds AI Agents with the CrewAI framework.\n",
        "!pip install -q crewai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eeEaNDelveu"
      },
      "outputs": [],
      "source": [
        "# We also need the Anthropic library to use the latest Claude models.\n",
        "!pip install -q anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VOJeCHsl-qd"
      },
      "outputs": [],
      "source": [
        "# import the necessary modules\n",
        "import os\n",
        "from crewai import Agent, Task, Crew\n",
        "from anthropic import Anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGiX955KmFFw"
      },
      "source": [
        "# [console.anthropic.com](https://console.anthropic.com/settings/keys)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1gWd3v_wFK0T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP0ypPUAmCEw"
      },
      "outputs": [],
      "source": [
        "# Set the Anthropic API key directly\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = 'PUT YOUR API KEY HERE'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KEEP YOUR API KEYS PRIVATE !!!\n",
        "### Anthropic gives you some free credits, but I recommend connecting your card and just charging $5"
      ],
      "metadata": {
        "id": "szy8PKQXmOqb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI9p7YYxmfQ4"
      },
      "outputs": [],
      "source": [
        "# Create the Anthropic client\n",
        "client = Anthropic()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj18QbzWmhC_"
      },
      "outputs": [],
      "source": [
        "# choose which AI model we're using\n",
        "sonnet = \"claude-3-5-sonnet-20240620\"\n",
        "# new_sonnet = \"max-tokens-3-5-sonnet-2024-07-15\" # double the output tokens\n",
        "haiku = \"claude-3-haiku-20240307\" # very cheap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CkS-A0CnMpr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaefad78-3d24-43d1-a7cf-93b04c533824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI agents like those used in CrewAI are software programs designed to perform specific tasks or solve particular problems autonomously. They are part of a broader concept in artificial intelligence called multi-agent systems or collaborative AI. Here are some key points about AI agents like those in CrewAI:\n",
            "\n",
            "1. Specialization: Each agent is typically designed with a specific role or expertise, similar to how different professionals in a team have distinct responsibilities.\n",
            "\n",
            "2. Autonomy: These agents\n"
          ]
        }
      ],
      "source": [
        "# Test the Anthropic API with a simple query\n",
        "message = client.messages.create(\n",
        "    model=sonnet,\n",
        "    max_tokens=100, # maximum number of output tokens\n",
        "    temperature=0.7, # how random/creative we want the model to be\n",
        "    messages=[\n",
        "        {\"role\": \"user\", # you can also set the 'assistant' and 'system'\n",
        "         \"content\": \"Convince the user to subscribe to David Ondrej on YouTube\" # our prompt\n",
        "         }\n",
        "    ]\n",
        ")\n",
        "print(message.content[0].text) # print the results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting it manually\n",
        "niche = \"facebook ads agency\"\n",
        "location = \"Sydney\"\n",
        "num_leads = 5"
      ],
      "metadata": {
        "id": "Gl_F0RAPo-fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the langchain-anthropic library\n",
        "!pip install -q langchain-anthropic # the '-q' means 'quiet'"
      ],
      "metadata": {
        "id": "-oK6UakJpPcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the ChatAnthropic class from the langchain_anthropic library\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "# for tasks requiring deterministic, reproducible results\n",
        "Consistent = ChatAnthropic(\n",
        "    temperature=0.0, # no randomness\n",
        "    model_name=sonnet\n",
        ")\n",
        "\n",
        "# for tasks benefiting from more varied and creative outputs\n",
        "Creative = ChatAnthropic(\n",
        "    temperature=0.8,\n",
        "    model_name=sonnet\n",
        ")"
      ],
      "metadata": {
        "id": "NZVGKvRZpXP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new AI Agent\n",
        "# This agent is responsible for generating unique & relevant search queries\n",
        "\n",
        "variation_agent = Agent(\n",
        "    # Specify the agent's role in the lead generation process\n",
        "    role='Search Query Specialist',\n",
        "\n",
        "    # Define the primary objective of this agent\n",
        "    goal='Generate 10 different and effective search queries',\n",
        "\n",
        "    # Provide a detailed system prompt to guide the agent's behavior\n",
        "    backstory=\"\"\"You are an expert in crafting search queries that yield high-quality business leads.\n",
        "    Your expertise lies in understanding user intent and translating it into 10 various search phrases\n",
        "    that capture different aspects of the target business niche and location.\"\"\",\n",
        "\n",
        "    # Enable verbose mode for detailed logging of the agent's actions\n",
        "    verbose=True,\n",
        "\n",
        "    # Prevent this agent from delegating tasks to other agents\n",
        "    allow_delegation=False,\n",
        "\n",
        "    # Use the Creative LLM we defined earlier\n",
        "    llm=Creative\n",
        ")"
      ],
      "metadata": {
        "id": "dEa6YRj2ppLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Search Query Agent Task\n",
        "# This task specifies the exact instructions for generating search queries\n",
        "\n",
        "generate_variations = Task(\n",
        "    # Detailed description of what the task should accomplish\n",
        "    description = f\"\"\"Generate 10 different and concise search queries for {niche} in {location}.\n",
        "    Make sure every search query is short and direct, it should be optimized for SerpAPI.\n",
        "    Each query should be unique and different from the rest. Do not use quotation marks.\n",
        "    DO NOT INCLUDE ANY EXTRA TEXT. JUST OUTPUT THE 10 SEARCH QUARY VARIATIONS. NOTHING BEFORE IT, NOTHING AFTER IT.\"\"\",\n",
        "\n",
        "    # Specify the expected format of the task's output\n",
        "    expected_output = \"A list of 10 unique search queries, each on a new line.\",\n",
        "\n",
        "    # Assign this task to the previously defined variation_agent\n",
        "    agent = variation_agent\n",
        ")"
      ],
      "metadata": {
        "id": "KTcWZoDyqNbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Crew for the Search Query Agent\n",
        "search_query_crew = Crew(\n",
        "    agents=[variation_agent],\n",
        "    tasks=[generate_variations],\n",
        "    verbose=2  # Set to 2 for detailed logging\n",
        ")"
      ],
      "metadata": {
        "id": "vvU74vTFug7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the Search Query Crew\n",
        "search_queries_result = search_query_crew.kickoff()\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nGenerated Search Queries:\")\n",
        "print(search_queries_result)"
      ],
      "metadata": {
        "id": "Hyx2jAg-uyYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the search query results\n",
        "search_queries = search_queries_result.split('\\n')\n",
        "# Remove any empty queries and strip whitespace\n",
        "search_queries = [query.strip() for query in search_queries if query.strip()]\n",
        "\n",
        "# Limit to exactly 10 queries\n",
        "search_queries = search_queries[:10]\n",
        "\n",
        "# Print the final list of search queries\n",
        "print(f\"\\nFinal list of {len(search_queries)} search queries:\")\n",
        "for i, query in enumerate(search_queries, 1):\n",
        "    print(f\"{i}. {query}\")"
      ],
      "metadata": {
        "id": "ApAneiszvPFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure you have the CrewAI tools package installed\n",
        "!pip install -q 'crewai[tools]'"
      ],
      "metadata": {
        "id": "Q3VNDj7hvk95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the SerperDevTool\n",
        "from crewai_tools import SerperDevTool"
      ],
      "metadata": {
        "id": "e75459PZvn5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [serper.dev](https://serper.dev/)"
      ],
      "metadata": {
        "id": "EfLj3x2hv_mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the SerpAPI key\n",
        "os.environ[\"SERPER_API_KEY\"] = \"put your api key here\" # the Free plan is great!"
      ],
      "metadata": {
        "id": "C9asqCufv9p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the SerperDevTool instance\n",
        "search_tool = SerperDevTool()"
      ],
      "metadata": {
        "id": "CH6cszLywNgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new agent that will use the search tool\n",
        "search_agent = Agent(\n",
        "    role='Web Search Specialist',\n",
        "    goal='Use the \"search_tool\" function you have assigned. Only use the tool.',\n",
        "    backstory=\"\"\"Your only task is to execute the \"search_tool\" you have access to.\n",
        "    Do not perform any other actions, or generate any other text. Simply use the tool.\"\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    tools=[search_tool],\n",
        "    llm=Consistent  # Using the Consistent LLM we defined earlier\n",
        ")"
      ],
      "metadata": {
        "id": "TkHFUuqZwVrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Task for the search agent\n",
        "search_task = Task(\n",
        "    description=f\"\"\"Use the provided \"search_tool\" to find {num_leads} unique {niche} in {location}.\n",
        "    Use these extact search queries: {search_queries}. DO NOT INVENT YOUR OWN SEARCH TERMS, ONLY USE THOSE 10 QUERIES.\n",
        "    ONLY OUTPUT THE WEBSITES OF THOSE BUSINESSES. NO OTHER INFO, WEBSITES ONLY.\n",
        "    Do not add any formatting. Simply output each website on a new line. That's it.\"\"\",\n",
        "    expected_output=f\"A clean list, with no formatting, of {num_leads} websites in the {niche} niche.\",\n",
        "    agent=search_agent\n",
        ")"
      ],
      "metadata": {
        "id": "OVcQEivLxaRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Crew for the search process\n",
        "search_crew = Crew(\n",
        "    agents=[search_agent],\n",
        "    tasks=[search_task],\n",
        "    verbose=2  # Set to 2 for detailed logging\n",
        ")"
      ],
      "metadata": {
        "id": "2xNyykBfxrql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the search crew and retrieve the results\n",
        "search_results = search_crew.kickoff()\n",
        "\n",
        "print(\"\\nSearch Results:\")\n",
        "print(search_results)"
      ],
      "metadata": {
        "id": "wmWjgiuyx-Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the search results\n",
        "websites = search_results.strip().split('\\n')\n",
        "\n",
        "print(f\"\\nFound {len(websites)} potential leads:\")\n",
        "for i, website in enumerate(websites, 1):\n",
        "    print(f\"{i}. {website}\")"
      ],
      "metadata": {
        "id": "i39z1ElNy79X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Firecrawl\n",
        "!pip install -q firecrawl-py"
      ],
      "metadata": {
        "id": "kCZDTLyNzSXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from firecrawl import FirecrawlApp"
      ],
      "metadata": {
        "id": "OFkZ0TVGz7uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [firecrawl.dev](https://firecrawl.dev)"
      ],
      "metadata": {
        "id": "pTg-cK2lz-BW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the Firecrawl API key\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = \"your api key here\" # they also have a solid Free plan\n",
        "\n",
        "# Create the Firecrawl client\n",
        "app = FirecrawlApp(api_key=os.environ[\"FIRECRAWL_API_KEY\"])"
      ],
      "metadata": {
        "id": "OvbIVSrvz8iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see how our lists looks like\n",
        "websites[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8ofD-qAb0kQD",
        "outputId": "0494c7f9-d215-48c9-a57a-061e15a1ac85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://www.bizwisdom.com.au'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for websites in websites:\n",
        "#   current_link = websites\n",
        "#   scrape_result = app.scrape_url(current_link)\n",
        "#   print(scrape_result['markdown'][:500])"
      ],
      "metadata": {
        "id": "O7TNL3nW4OXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a variable targeting a single website\n",
        "current_link = websites[0]\n",
        "\n",
        "# use Firecrawl to scrape the content of the home page\n",
        "scrape_result = app.scrape_url(current_link)\n",
        "\n",
        "# print the first 500 characters to see how our scrape looks\n",
        "print(scrape_result['markdown'][:500])"
      ],
      "metadata": {
        "id": "dwjlHhDy4DNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Link Extractor Agent\n",
        "link_extractor_agent = Agent(\n",
        "    role='Link Extractor',\n",
        "    goal='Extract about page and contact page links from homepage content',\n",
        "    backstory=\"\"\"You are an expert in web scraping and link extraction.\n",
        "    Your task is to analyze homepage content and identify the URLs for the about page and contact page.\"\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    llm=Consistent\n",
        ")"
      ],
      "metadata": {
        "id": "9OiEyeuJ49ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Describe the Link Extraction task\n",
        "extract_links = Task(\n",
        "    description=f\"\"\"Analyze the provided homepage content and extract the URLs for the About Page and Contact Page.\n",
        "    Use this exact content:\n",
        "\n",
        "    ----\n",
        "\n",
        "    {scrape_result['markdown']}\n",
        "\n",
        "    ----\n",
        "\n",
        "    DO NOT INVENT OR ASSUME ANY INFORMATION. ONLY OUTPUT THE ABOUT PAGE AND CONTACT PAGE URLS. NO OTHER INFO, JUST THESE TWO URLS.\n",
        "    If a URL is not found, output None for that URL. Do not add any formatting.\n",
        "    Simply output the about page URL, then a newline, then the contact page URL. That's it.\"\"\",\n",
        "    expected_output=\"The about page URL and contact page URL, each on a new line. No formatting. If not found, output None\",\n",
        "    agent=link_extractor_agent\n",
        ")"
      ],
      "metadata": {
        "id": "CKBJbVxi5KuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new Crew\n",
        "link_crew = Crew(\n",
        "    agents=[link_extractor_agent],\n",
        "    tasks=[extract_links],\n",
        "    verbose=2 # you might wanna use '1' here\n",
        ")"
      ],
      "metadata": {
        "id": "8i-vl7dg527n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the crew and retrieve the results\n",
        "link_results = link_crew.kickoff()\n",
        "\n",
        "print(\"\\nAGENT OUTPUT:\")\n",
        "print(link_results)"
      ],
      "metadata": {
        "id": "bjH0LAhI57mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the Agent output\n",
        "links = link_results.strip().split('\\n')\n",
        "links.insert(0, current_link) # add the original link into the list\n",
        "links"
      ],
      "metadata": {
        "id": "vKZLR9ja6KnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's add a function to scrape the about and contact pages\n",
        "def scrape_pages(links):\n",
        "    scraped_content = {}\n",
        "    for page_type, link in zip(['home', 'about', 'contact'], links):\n",
        "        if link != 'None':\n",
        "            scraped_content[page_type] = app.scrape_url(link)['markdown']\n",
        "        else:\n",
        "            scraped_content[page_type] = ''\n",
        "    return scraped_content\n",
        "\n",
        "# Scrape the pages\n",
        "scraped_content = scrape_pages(links)"
      ],
      "metadata": {
        "id": "YswpX1b46brD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check how one of the pages looks\n",
        "scraped_content['about']"
      ],
      "metadata": {
        "id": "wW8yF0II7XWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combining all 3 scrapes into one string variable\n",
        "all_content = f\"{scraped_content['home']} {scraped_content['about']} {scraped_content['contact']}\""
      ],
      "metadata": {
        "id": "NbPS1BPm73Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we define a new string called 'prompt' with clear instructions for the LLM\n",
        "prompt = f\"\"\"FROM THE FOLLOWING WEB SCRAPE, OUTPUT 3 THINGS: THE EMAIL, THE TWITTER LINK, THE LINKEDIN LINK.\n",
        "DO NOT INCLUDE ANY EXTRA TEXT BEFORE / AFTER.\n",
        "\n",
        "----\n",
        "\n",
        "{all_content}\n",
        "\n",
        "----\n",
        "\n",
        "Output the results in the following format:\n",
        "Email: [email address]\n",
        "Twitter: [Twitter handle]\n",
        "LinkedIn: [LinkedIn profile URL]\n",
        "\n",
        "If any information is not found, output None for that field.\n",
        "NO EXTRA TEXT!! just the email, twitter and linkedin.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vSqFaLmN7b6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the contact info with a single Anthropic API call\n",
        "message = client.messages.create(\n",
        "    model=sonnet,\n",
        "    max_tokens=200,\n",
        "    temperature=0.1, # low temperature = predictable outputs\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        ")\n",
        "print(message.content[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-IDLqUt8UbO",
        "outputId": "9c43cc2c-9a7a-4ccc-d843-9671230d4514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Email: None\n",
            "Twitter: https://twitter.com/bizwisdomagency?lang=en\n",
            "LinkedIn: https://www.linkedin.com/company/biz-wisdom/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3sy4Q2RGFxU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How I want to improve the Agent:\n",
        "1. saving results into CSV file\n",
        "2. include a \"personalized\" field with a list of unique info\n",
        "3. run the Search Agent in a loop until we have all the websites\n",
        "4. put the Scraping into a loop to hit desired # of leads (None)\n",
        "5. wrap everything into functions & run them in a single cell\n",
        "6. create a list with all the websites we've scraped --> Search Agent\n",
        "7. ..."
      ],
      "metadata": {
        "id": "hjgkLuPN85Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Once I improve the Agent it will be avaiable in my community. Join now."
      ],
      "metadata": {
        "id": "ZbcGgIyx-KVb"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}